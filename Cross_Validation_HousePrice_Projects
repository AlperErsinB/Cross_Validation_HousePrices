import pandas as pd
import numpy as np
from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score
from sklearn.linear_model import LinearRegression, LogisticRegression



train_data = pd.read_csv('data/housing/train.csv')



train_data.head()


# eksik verili satırları sil

train_data.dropna(axis=0, subset=['SalePrice'], inplace=True)



train_data.head()


# içinde null değerler olan sütunları sil

train_data.drop(['LotFrontage', 'GarageYrBlt', 'MasVnrArea', 'Alley', 'PoolQC', 'MiscFeature'], 
                 axis=1, inplace=True)
                 
                 
                 
train_data.head()
 


# sonuç değişkenini al

y = train_data.SalePrice

# sonuç değişkenini çıkar
train_data.drop(['SalePrice'], axis=1, inplace=True)




# sadece numerik kolonları seç

numeric_cols = [cname for cname in train_data.columns if train_data[cname].dtype in ['int64', 'float64']]

X = train_data[numeric_cols].copy()



print("Input datanın şekli: {} ve sonuç değişkenin şekli: {}".format(X.shape, y.shape))



# Diyelim K = 5 olsun 
# K -> n_splits

kf = KFold(n_splits = 5, shuffle=True, random_state=42)
kf



cnt = 1

# split() methodu Train-Test olarak ayırmak için bize indeksleri döner
for train_index, test_index in kf.split(X, y):
    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')
    cnt += 1
    
    
# RMSE'yi hesaplamak için - (eksi) ile çarpacağız.
# cross_val_score() dan bize negatif gelecek

def rmse(score):
    rmse = np.sqrt(-score)
    print(f'rmse = {"{:.2f}".format(rmse)}')
    
    
# Lineer Regresyon modeli

# score için verdiğimiz değer MSE'nin negatifi, çünkü MSE aslında maliyet demek
# yüksek MSE istemediğimiz için negatif olarak verdik
score = cross_val_score(LinearRegression(), X, y, cv = kf, scoring="neg_mean_squared_error")



print(f'Her bir fold için skor: {score}')

rmse(score.mean())



np.sqrt(-sum(score) / len(score))



train_data = pd.read_csv('data/titanic/train.csv')



# eksik verili satırları sil

train_data.dropna(axis=0, subset=['Survived'], inplace=True)



train_data.head()



# sonuç değişkeni

y = train_data.Survived
y



# train datadan y'yi çıkar

train_data.drop(['Survived'], axis=1, inplace=True)



train_data.head()


# İçinde Null değerler olan Age sütununu sil

train_data.drop(['Age'], axis=1, inplace=True)



train_data.head()


# sadece numerik kolonları seç

numeric_cols = [cname for cname in train_data.columns if train_data[cname].dtype in ['int64', 'float64']]



X = train_data[numeric_cols].copy()



X.head()



print("Train datanın şekli: {} ve sonuç değişkenin şekli: {}".format(X.shape, y.shape))



# ilk 5 train datasını göster

pd.concat([X, y], axis=1).head()



# K = 5
# Stratified K-Fold yaptığımız için foldlardaki sınıf oranları gerçek dataya ile yakın olacaktır.

kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)




kf



cnt = 1

# split()  ile indexler
for train_index, test_index in kf.split(X, y):
    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')
    cnt+=1
    
    
    
# CV skorunu alalım
# skorlama yöntemi -> accuracy

score = cross_val_score(LogisticRegression(random_state= 42), X, y, cv = kf, scoring="accuracy")


print(f"Her bir fold'un skoru: {score}")
print(f'Ortalama Skor: {"{:.2f}".format(score.mean())}')



score.mean()



# Logistic Regression'ın bütün Solver'larını deneylim
solvers = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']


# her bir solver için ortalama score hesaplayalım
# max_iter'i 4000 verdik
for solver in solvers:
    
    score = cross_val_score(LogisticRegression(max_iter=4000, solver=solver, random_state=42), 
                            X, y, cv=kf, scoring="accuracy")
    
    print(f'Ortalama Skor({solver}): {"{:.3f}".format(score.mean())}')
    
    


 
